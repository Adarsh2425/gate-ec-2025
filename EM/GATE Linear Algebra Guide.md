. Matrices: Types & Properties**

| Type              | Definition                | Properties                                                                                                                                      | Eigenvalues                                  | Determinant                    | Other                               |
| ----------------- | ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------- | ------------------------------ | ----------------------------------- |
| Square            | `n x n`                  |                                                                                                                                                 |                                               |                                |                                     |
| Diagonal          | Non-diagonal = 0          | `A‚Åø` = (diagonals raised to n)                                                                                                                | Diagonal elements                           | Product of diagonal elements   |                                     |
| Identity (I)      | Diagonal = 1, others = 0  | `AI = IA = A`                                                                                                                                  | All 1s                                        | 1                              |                                     |
| Zero (0)        | All elements = 0          |                                                                                                                                           |  All 0s                                    |0                                  |          |
| Symmetric         | `A·µÄ = A`                 | Eigenvectors for distinct eigenvalues are orthogonal; diagonalizable                                                                           | Real                                          |                                |                                     |
| Skew-Symmetric    | `A·µÄ = -A`                | Diagonal elements are 0                                                                                                                         | 0 or purely imaginary                        | 0 (if odd order),  >=0 (even order)| `A¬≤` is symmetric                 |
| Orthogonal        | `A·µÄA = AA·µÄ = I`          | `A‚Åª¬π = A·µÄ`; preserves lengths and angles; rows/cols are orthonormal                                                                               | Magnitude 1 (¬±1 if real)                    | ¬±1                             |                                     |
| Upper Triangular  | Below diagonal = 0       |                                                                                                                                                 | Diagonal elements                           | Product of diagonal elements   |                                     |
| Lower Triangular  | Above diagonal = 0       |                                                                                                                                                 | Diagonal elements                           | Product of diagonal elements   |                                     |
| Idempotent        | `A¬≤ = A`                 |                                                                                                                                                 | 0 or 1                                        |                                |                                     |
| Nilpotent         | `A·µè = 0` for some `k`     |                                                                                                                                                 | All 0s                                        | 0                              |                                     |
| Involutory        | `A¬≤ = I`                 |                                                                                                                                                  | +1 or -1                                         |                     |         |
|Singular           |                           |                                                                                                                                          |At least one zero                               |0  |  Not Invertible                    |
|Non-Singular       |                           |                                                                                                                                    |   None Zero                                   |Non-Zero      |      Invertible               |
| Hermitian        | `A = A·¥¥`                | Generalization of symmetric to complex matrices.                                                                                                 | Real                                      |                                    |
| Unitary     |`A·¥¥A = I`           | Generalization of orthogonal to complex matrices.                                                      | Magnitude 1                         | |det(A)| = 1               |                                              |

üî• **Super-Detailed Ultimate GATE Linear Algebra Guide** üî•

This guide is designed for complete mastery. It covers everything from basic definitions to advanced problem-solving techniques.

üìå **1. Vector Spaces and Subspaces**

*   **Vector Space (Detailed Definition):**
    *   A set `V` (whose elements are called "vectors") over a field `F` (usually `R` or `C`) equipped with two operations:
        *   **Vector Addition (+):**  Takes two vectors `u, v ‚àà V` and produces another vector `u + v ‚àà V`.
        *   **Scalar Multiplication (¬∑):** Takes a scalar `c ‚àà F` and a vector `v ‚àà V` and produces another vector `cv ‚àà V`.
    *   These operations *must* satisfy the following eight axioms for all `u, v, w ‚àà V` and `a, b ‚àà F`:
        1.  **Associativity of Addition:** `(u + v) + w = u + (v + w)`
        2.  **Commutativity of Addition:** `u + v = v + u`
        3.  **Identity Element of Addition:** There exists a `0 ‚àà V` (the *zero vector*) such that `v + 0 = v` for all `v ‚àà V`.
        4.  **Inverse Elements of Addition:** For every `v ‚àà V`, there exists a `-v ‚àà V` (the *additive inverse* of `v`) such that `v + (-v) = 0`.
        5.  **Compatibility of Scalar Multiplication with Field Multiplication:** `a(bv) = (ab)v`
        6.  **Identity Element of Scalar Multiplication:** `1v = v` (where `1` is the multiplicative identity in `F`)
        7.  **Distributivity of Scalar Multiplication with respect to Vector Addition:** `a(u + v) = au + av`
        8.  **Distributivity of Scalar Multiplication with respect to Field Addition:** `(a + b)v = av + bv`
    *   **Common Fields (F):**
        *   `R`: The field of real numbers.
        *   `C`: The field of complex numbers.
        *   `Q`: The field of rational numbers. (Less common in GATE, but good to know)

*   **Subspace (Detailed Definition):**
    *   A *non-empty* subset `W` of a vector space `V` is a subspace if `W` is *itself* a vector space under the *same* addition and scalar multiplication operations defined on `V`.
    *   **Subspace Theorem (The 3-Point Check):** A non-empty subset `W` of a vector space `V` is a subspace if and only if *all three* of these conditions hold:
        1.  **Zero Vector:** `0 ‚àà W` (The zero vector of `V` must be in `W`)
        2.  **Closure under Addition:** If `u ‚àà W` and `v ‚àà W`, then `u + v ‚àà W`.
        3.  **Closure under Scalar Multiplication:** If `u ‚àà W` and `c ‚àà F`, then `cu ‚àà W`.
    *   **Why these 3 conditions are sufficient:** If these conditions hold, all the other vector space axioms are *automatically* satisfied because they are "inherited" from the larger vector space `V`.

*   **Examples (More Detail):**
    *   **Vector Spaces:**
        *   `R‚Åø`:  `{(x‚ÇÅ, x‚ÇÇ, ..., x‚Çô) | x·µ¢ ‚àà R}` (n-tuples of real numbers)
        *   `C‚Åø`:  `{(z‚ÇÅ, z‚ÇÇ, ..., z‚Çô) | z·µ¢ ‚àà C}` (n-tuples of complex numbers)
        *   `M_(m,n)(R)`:  All `m x n` matrices with real entries.
        *   `P‚Çô(R)`:  All polynomials with real coefficients and degree ‚â§ `n`.  Example: `p(x) = 3x¬≤ + 2x - 1` is in `P‚ÇÇ(R)`.
        *   `F(R)`:  All functions `f: R -> R`.
        *   `C(R)`:  All *continuous* functions `f: R -> R`.  (This is a subspace of `F(R)`).
        *   The set of all solutions to a *homogeneous* linear differential equation.
    *   **Subspaces:**
        *   `{(x, y, 0) | x, y ‚àà R}`:  The xy-plane within `R¬≥`.
        *   `{A ‚àà M_(n,n)(R) | A = A·µÄ}`:  The set of all symmetric `n x n` matrices (a subspace of `M_(n,n)(R)`).
        *   `{A ‚àà M_(n,n)(R) | tr(A) = 0}`:  The set of all `n x n` matrices with trace 0.
        *   `{p(x) ‚àà P‚Çô(R) | p(1) = 0}`:  All polynomials in `P‚Çô(R)` that have a root at `x = 1`.
        *   The *span* of any set of vectors is *always* a subspace.
    * **Non-Subspaces** (and why):
       * `{ (x,y) | x+y =1}` Not contain (0,0)
       *  `{A ‚àà M_(n,n)(R) | det(A) = 0}`:  The set of singular matrices.  *Not* closed under addition.
       * `{ (x,y) | x,y >=0}` Not closed under scalar multiplication.

*   **Span (Definition):**
    *   The *span* of a set of vectors `S = {v‚ÇÅ, v‚ÇÇ, ..., v‚Çñ}` in a vector space `V` is the set of *all possible linear combinations* of those vectors:
        `span(S) = {c‚ÇÅv‚ÇÅ + c‚ÇÇv‚ÇÇ + ... + c‚Çñv‚Çñ | c·µ¢ ‚àà F}`
    *   **Key Property:**  `span(S)` is *always* a subspace of `V`, even if `S` is not linearly independent.

*   **GATE Problem-Solving (Vector Spaces & Subspaces):**
    1.  **Is a set a vector space?**  (Rarely asked directly, but understanding is crucial).  Theoretically, you'd check all 8 axioms.  Practically, focus on closure.
    2.  **Is a subset a subspace?**  Use the 3-point check: zero vector, closure under addition, closure under scalar multiplication.  Start with the zero vector ‚Äì it's the fastest way to rule out many sets.
    3.  **Finding a spanning set:** If the subspace is defined by equations, solve the equations and express the general solution in terms of free variables.

* **Challenging Concepts & Examples:**
Direct Sum, Quotient Space

üìå **2. Linear Independence, Basis, and Dimension**

*   **Linear Combination (Review):**  A vector `v` is a linear combination of `v‚ÇÅ, v‚ÇÇ, ..., v‚Çñ` if `v = c‚ÇÅv‚ÇÅ + c‚ÇÇv‚ÇÇ + ... + c‚Çñv‚Çñ` for some scalars `c·µ¢`.

*   **Linear Independence (Detailed Definition):**
    *   A set of vectors `{v‚ÇÅ, v‚ÇÇ, ..., v‚Çñ}` is *linearly independent* if the *only* solution to the equation
        `c‚ÇÅv‚ÇÅ + c‚ÇÇv‚ÇÇ + ... + c‚Çñv‚Çñ = 0`
        is the *trivial solution*: `c‚ÇÅ = c‚ÇÇ = ... = c‚Çñ = 0`.
    *   If there exists a *non-trivial* solution (at least one `c·µ¢ ‚â† 0`), the set is *linearly dependent*.  This means at least one vector can be written as a linear combination of the others.

*   **Basis (Detailed Definition):**
    *   A set of vectors `B = {v‚ÇÅ, v‚ÇÇ, ..., v‚Çô}` is a *basis* for a vector space `V` if:
        1.  `B` is *linearly independent*.
        2.  `span(B) = V` (`B` spans `V`, meaning every vector in `V` can be written as a linear combination of vectors in `B`).
    *   **Key Property:**  Every vector in `V` can be expressed as a *unique* linear combination of basis vectors.

*   **Dimension (Detailed Definition):**
    *   The *dimension* of a vector space `V` (denoted `dim(V)`) is the number of vectors in *any* basis for `V`.  This number is the same for *all* possible bases of `V`.
    *   If `V = {0}` (the vector space containing only the zero vector), then `dim(V) = 0`.
    *   If `V` does not have a finite basis, it is called *infinite-dimensional* (e.g., the space of all polynomials, `F(R)`).  GATE primarily deals with finite-dimensional spaces.

*   **Key Theorems & Proofs (More Detail):**
    *   **Spanning Set Theorem:** If `S = {v‚ÇÅ, v‚ÇÇ, ..., v‚Çñ}` spans a vector space `V`, then *some subset* of `S` is a basis for `V`.
        *   **Proof Idea:**  If `S` is linearly independent, it's already a basis.  If not, there's a non-trivial linear combination equal to zero.  This means you can express one vector in terms of the others and remove it from `S`.  Keep repeating this process until you get a linearly independent subset that still spans `V`.
    *   **Linearly Independent Set Theorem:**  If `S = {v‚ÇÅ, v‚ÇÇ, ..., v‚Çñ}` is a linearly independent set in a vector space `V`, and `dim(V) = n`, then `S` can be *extended* to a basis for `V` (by adding `n - k` more linearly independent vectors).
        *   **Proof Idea:** If `span(S) = V`, then `S` is a basis.  If not, there's a vector `v` in `V` that's not in `span(S)`.  The set `{v‚ÇÅ, v‚ÇÇ, ..., v‚Çñ, v}` is still linearly independent.  Keep adding vectors in this way until you have `n` linearly independent vectors, which must form a basis.
    *   **Invariance of Dimension (Proof):** If `B‚ÇÅ` and `B‚ÇÇ` are both bases for `V`, they have the same number of vectors.
        *    **Proof:**
            Suppose `B‚ÇÅ = {u‚ÇÅ, u‚ÇÇ, ..., u‚Çò}` and `B‚ÇÇ = {v‚ÇÅ, v‚ÇÇ, ..., v‚Çô}`.
            Assume, for the sake of contradiction, that `m < n`.
            Since `B‚ÇÅ` is a basis, it spans `V`. Therefore, each vector in `B‚ÇÇ` can be written as a linear combination of vectors in `B‚ÇÅ`:
           `v·µ¢ = a‚ÇÅ·µ¢u‚ÇÅ + a‚ÇÇ·µ¢u‚ÇÇ + ... + a‚Çò·µ¢u‚Çò`  for `i = 1, 2, ..., n`
            Consider the equation: `c‚ÇÅv‚ÇÅ + c‚ÇÇv‚ÇÇ + ... + c‚Çôv‚Çô = 0`
            Substituting the expressions for `v·µ¢`:
            `c‚ÇÅ(a‚ÇÅ‚ÇÅu‚ÇÅ + ... + a‚Çò‚ÇÅu‚Çò) + ... + c‚Çô(a‚ÇÅ‚Çôu‚ÇÅ + ... + a‚Çò‚Çôu‚Çò) = 0`
            Rearrange to group terms by `u·µ¢`:
           `(c‚ÇÅa‚ÇÅ‚ÇÅ + ... + c‚Çôa‚ÇÅ‚Çô)u‚ÇÅ + ... + (c‚ÇÅa‚Çò‚ÇÅ + ... + c‚Çôa‚Çò‚Çô)u‚Çò = 0`
            Since `B‚ÇÅ` is linearly independent, all coefficients of `u·µ¢` must be zero:
         `c‚ÇÅa‚ÇÅ‚ÇÅ + ... + c‚Çôa‚ÇÅ‚Çô = 0`
              ...
           `c‚ÇÅa‚Çò‚ÇÅ + ... + c‚Çôa‚Çò‚Çô = 0`
            This is a homogeneous system of `m` equations with `n` unknowns (`c·µ¢`).  Since `m < n`, there are more unknowns than equations, which guarantees a *non-trivial* solution for the `c·µ¢`.  This contradicts the assumption that `B‚ÇÇ` is linearly independent.
           Therefore, `m ‚â• n`. By symmetry (swapping the roles of `B‚ÇÅ` and `B‚ÇÇ`), we also get `n ‚â• m`. Thus, `m = n`.

*   **GATE Problem-Solving (Linear Independence, Basis, Dimension):**
    1.  **Checking Linear Independence:**
        *   **Determinant Method (Square Matrices Only):** Form a matrix with the vectors as *columns*.  If the determinant is *non-zero*, the vectors are linearly independent.  If the determinant is *zero*, they are linearly dependent.
        *   **Row Reduction Method (General):** Form a matrix with the vectors as *columns* (or rows).  Row reduce to echelon form.  If there are *no free variables* (a pivot in every column), the vectors are linearly independent. If there are free variables, they are linearly dependent.
    2.  **Finding a Basis:**
        *   **From a Spanning Set:**  Form a matrix with the vectors as *columns*. Row reduce to echelon form.  The *original* columns corresponding to the pivot columns in the echelon form are a basis for the *column space* (which is the span of the original vectors).
        * **Row Space**: The non zero rows of RREF will form the basis.
        * **Column Space**: The pivot columns of the original metrix will form the basis of the column space.
    3.  **Finding Dimension:**  The dimension is equal to the number of vectors in *any* basis. This is also equal to the *rank* of the matrix formed by the vectors.

*   **Challenging Concepts & Examples:**
    *   **Coordinates with Respect to a Basis:** If `B = {v‚ÇÅ, v‚ÇÇ, ..., v‚Çô}` is a basis for `V`, and `v ‚àà V`, the *coordinates* of `v` with respect to `B` are the unique scalars `c‚ÇÅ, c‚ÇÇ, ..., c‚Çô` such that `v = c‚ÇÅv‚ÇÅ + c‚ÇÇv‚ÇÇ + ... + c‚Çôv‚Çô`.  The coordinate vector is written as `[v]_B = [c‚ÇÅ, c‚ÇÇ, ..., c‚Çô]·µÄ`.
    *  **Change of Basis**:

üìå **3. Matrices and Linear Transformations**

*   **Linear Transformation (Definition):**  A function `T: V -> W` (where `V` and `W` are vector spaces) is a *linear transformation* if it satisfies:
    1.  `T(u + v) = T(u) + T(v)` for all `u, v ‚àà V`
    2.  `T(cu) = cT(u)` for all `u ‚àà V` and `c ‚àà F`
    *   **Key Properties:**
        *   `T(0) = 0` (The zero vector in `V` maps to the zero vector in `W`)
        *   `T(c‚ÇÅv‚ÇÅ + c‚ÇÇv‚ÇÇ + ... + c‚Çñv‚Çñ) = c‚ÇÅT(v‚ÇÅ) + c‚ÇÇT(v‚ÇÇ) + ... + c‚ÇñT(v‚Çñ)` (Linear transformations preserve linear combinations)

*   **Matrix Representation of Linear Transformations:**
    *   If `V` and `W` are finite-dimensional, every linear transformation `T: V -> W` can be represented by a matrix.
    *   Let `B = {v‚ÇÅ, v‚ÇÇ, ..., v‚Çô}` be a basis for `V` and `C = {w‚ÇÅ, w‚ÇÇ, ..., w‚Çò}` be a basis for `W`.  The matrix of `T` with respect to these bases, denoted `[T]_(C<-B)` (or sometimes just `[T]`), is an `m x n` matrix whose columns are the coordinate vectors of `T(v‚ÇÅ), T(v‚ÇÇ), ..., T(v‚Çô)` with respect to the basis `C`.
        *   That is, the `j`-th column of `[T]_(C<-B)` is `[T(v‚±º)]_C`.
    *   If `x` is a vector in `V`, and `[x]_B` is its coordinate vector with respect to `B`, then:
        `[T(x)]_C = [T]_(C<-B) [x]_B`
        (The matrix of `T` times the coordinate vector of `x` gives the coordinate vector of `T(x)`).
    * **Standard Matrix:** If V = R‚Åø and W=R·µê and using standard basis then the metrix representation of T is called as Standard Matrix.

*   **Matrix Operations (Detailed):**
    *   **Addition (A + B):**  Element-wise addition.  `A` and `B` *must* have the same dimensions.
    *   **Scalar Multiplication (cA):** Multiply *each* element of `A` by the scalar `c`.
    *   **Matrix Multiplication (AB):**
        *   The `(i, j)`-th entry of `AB` is the dot product of the `i`-th row of `A` and the `j`-th column of `B`.
        *   **Compatibility:** The number of *columns* of `A` must equal the number of *rows* of `B`.  If `A` is `m x n` and `B` is `n x p`, then `AB` is `m x p`.
        *   **Not Commutative:** In general, `AB ‚â† BA`.
        *   **Associative:** `(AB)C = A(BC)`
        *   **Distributive:** `A(B + C) = AB + AC` and `(A + B)C = AC + BC`
    *   **Transpose (A·µÄ):**  Swap rows and columns.  The `(i, j)`-th entry of `A·µÄ` is the `(j, i)`-th entry of `A`.
        *   `(A·µÄ)·µÄ = A`
        *   `(A + B)·µÄ = A·µÄ + B·µÄ`
        *   `(cA)·µÄ = cA·µÄ`
        *   **(AB)·µÄ = B·µÄA·µÄ** (Important!)
    *   **Inverse (A‚Åª¬π):**
        *   *Only* square matrices can have inverses.
        *   A square matrix `A` is *invertible* (or *non-singular*) if there exists a matrix `A‚Åª¬π` such that `AA‚Åª¬π = A‚Åª¬πA = I` (where `I` is the identity matrix).
        *   `A` is invertible if and only if `det(A) ‚â† 0`.
        *   `(A‚Åª¬π)‚Åª¬π = A`
        *   `(cA)‚Åª¬π = (1/c)A‚Åª¬π` (if `c ‚â† 0`)
        *   **(AB)‚Åª¬π = B‚Åª¬πA‚Åª¬π** (Important!)
        *   `(A·µÄ)‚Åª¬π = (A‚Åª¬π )·µÄ`
    *   **Trace (tr(A)):**  The sum of the diagonal elements of a *square* matrix `A`.
        *   `tr(A + B) = tr(A) + tr(B)`
        *   `tr(cA) = c tr(A)`
        *   `tr(AB) = tr(BA)` (even if `AB ‚â† BA`)
        *   `tr(A·µÄ) = tr(A)`
        * `tr(P‚Åª¬πAP)=tr(A)`

*   **Determinant (Detailed):**
    *   **2x2 Matrix:** `det([[a, b], [c, d]]) = ad - bc`
    *   **3x3 Matrix:** Use cofactor expansion (or the "rule of Sarrus").
    *   **Larger Matrices:**  Use cofactor expansion (recursive definition) or row reduction to triangular form.
    *   **Properties (Extensive List):**
        *   `det(A·µÄ) = det(A)`
        *   `det(AB) = det(A)det(B)`
        *   `det(cA) = c‚Åødet(A)` (where `A` is `n x n`)
        *   `det(A‚Åª¬π) = 1/det(A)` (if `A` is invertible)
        *   Swapping *any* two rows (or columns) changes the *sign* of the determinant.
        *   Multiplying a row (or column) by a scalar `k` multiplies the determinant by `k`.
        *   Adding a multiple of one row (or column) to another *does not change* the determinant.
        *   If `A` has a row or column of zeros, `det(A) = 0`.
        *   If `A` has two identical rows or columns, `det(A) = 0`.
        *   The determinant of a triangular matrix (upper or lower) is the *product* of the diagonal entries.
        *   If `A` is orthogonal, `det(A) = ¬±1`.

*   **Important Matrix Types (Extensive List with Properties):**

    *   **Diagonal Matrix:** All non-diagonal entries are zero.  Easy to compute powers and inverses.
    *   **Triangular Matrices (Upper and Lower):**  Determinant is the product of diagonal entries.  Eigenvalues are the diagonal entries.
    *   **Symmetric Matrix (`A = A·µÄ`):**  *Real* eigenvalues. Eigenvectors corresponding to *distinct* eigenvalues are orthogonal.  Can be orthogonally diagonalized (`A = QDQ·µÄ`, where `Q` is orthogonal and `D` is diagonal).
    *   **Skew-Symmetric Matrix (`A = -A·µÄ`):** Diagonal entries are *zero*. Eigenvalues are purely imaginary or zero.
    *   **Orthogonal Matrix (`A·µÄA = AA·µÄ = I`, so `A‚Åª¬π = A·µÄ`):**  Preserves lengths and angles (rotations, reflections).  Columns (and rows) form an *orthonormal* set.  Eigenvalues have absolute value 1. Determinant is ¬±1.
    *   **Idempotent Matrix (`A¬≤ = A`):**  Eigenvalues are 0 or 1.  `rank(A) = tr(A)`.
    *   **Nilpotent Matrix (`A·µè = 0` for some positive integer `k`):** All eigenvalues are 0.
    * **Involutory**
    *   **Singular Matrix (`det(A) = 0`):**  Not invertible.  At least one eigenvalue is 0.
    *  **Hermitian:**
     * **Unitary:**

*   **GATE Problem-Solving (Matrices):**
    1.  **Matrix Multiplication:**  Practice, practice, practice.  Be mindful of the order!
    2.  **Determinants:**  Use properties to simplify before calculating.  Row operations (especially adding multiples of rows) are very helpful.
    3.  **Inverses:**  For 2x2, use the formula.  For larger matrices, Gaussian elimination (`[A | I] -> [I | A‚Åª¬π]`) is usually faster.  Recognize orthogonal matrices (`A‚Åª¬π = A·µÄ`).
    4. **Cayley Hamilton Theorem:**
    5.  **Recognize Special Matrices:**  Use their properties to save time. For example, if you see a symmetric matrix, you immediately know its eigenvalues are real.
    6. If matrix is in the form of `[[a, b], [b, a]]`

*Challenging Concepts*
*  **Block Matrices:**
* **Matrix Decompositions:**

üìå **4. Rank and Nullity**

*   **Rank (Detailed Definition):**  The rank of a matrix `A` (denoted `rank(A)`) is:
    *   The maximum number of *linearly independent rows* of `A`.
    *   The maximum number of *linearly independent columns* of `A`.
    *   The *dimension* of the *column space* of `A` (the space spanned by the columns of `A`).
    *   The *dimension* of the *row space* of `A` (the space spanned by the rows of `A`).
    *   The number of *non-zero rows* in the *row echelon form* of `A`.
    *   The number of *pivot columns* in the row echelon form of `A`.
    * **Key Point:** The row rank and column rank of a matrix are *always* equal.

* **Column Space (C(A)):** The set of all linear combinations of the *columns* of A. It is a subspace of R·µê (if A is m x n)
* **Row Space (C(A·µÄ)):** The set of all linear combinations of the *rows* of A.  It is a subspace of R‚Åø.

*   **Nullity (Detailed Definition):**  The nullity of a matrix `A` (denoted `nullity(A)`) is:
    *   The *dimension* of the *null space* of `A` (denoted `N(A)` or `null(A)`).
    *   The *null space* of `A` is the set of *all* solutions to the homogeneous equation `Ax = 0`.  `N(A) = {x ‚àà R‚Åø | Ax = 0}` (if A is m x n)
    *   The number of *free variables* in the solution to `Ax = 0`.

*   **Rank-Nullity Theorem (Proof):**  For any `m x n` matrix `A`:
    `rank(A) + nullity(A) = n`  (where `n` is the number of *columns* of `A`)
    *   **Proof Idea:**
       1. Let `r = rank(A)`.  This means there are `r` pivot columns in the row echelon form of `A`.
        2.  The remaining `n - r` columns correspond to free variables in the solution to `Ax = 0`.
        3.  The general solution to `Ax = 0` can be expressed as a linear combination of `n - r` linearly independent vectors (one for each free variable).
        4.  Therefore, the null space has dimension `n - r`, so `nullity(A) = n - r`.
        5.  Hence, `rank(A) + nullity(A) = r + (n - r) = n`.

* **Key Theorems and Properties (Rank and Nullity):**
    *   `rank(A) = rank(A·µÄ)`
    *   `rank(AB) ‚â§ min(rank(A), rank(B))`
    *   `rank(A + B) ‚â§ rank(A) + rank(B)`
    *   If `A` is `m x n`, then `rank(A) ‚â§ min(m, n)`.
    *  `rank(A) = rank(A·µÄA) = rank(AA·µÄ)`
    * **Sylvester's Rank Inequality:** For matrices A (m x n) and B (n x p),
          `rank(A) + rank(B) - n ‚â§ rank(AB) ‚â§ min(rank(A), rank(B))`

*   **GATE Problem-Solving (Rank and Nullity):**
    1.  **Finding Rank:**  Row reduce the matrix to echelon form. The number of non-zero rows (or pivot columns) is the rank.
    2.  **Finding Nullity:**  Solve the homogeneous system `Ax = 0`. The number of free variables is the nullity.  Alternatively, use the Rank-Nullity Theorem: `nullity(A) = n - rank(A)`.
    3.  **Relating Rank to Solutions of Linear Equations:**
        *   `Ax = b` has a solution if and only if `rank(A) = rank([A | b])`.
        *   If a solution exists, it's unique if `rank(A) = n` (number of unknowns).
        *   If a solution exists, there are infinitely many solutions if `rank(A) < n`.
    4. **Relating Rank to Invertibility:** A square matrix `A` is invertible if and only if `rank(A) = n` (full rank), which is equivalent to `det(A) ‚â† 0`.

*Challenging Concepts*
   * **Left Null Space:**

üìå **5. Eigenvalues and Eigenvectors**

*   **Definition:** Let `A` be an `n x n` matrix.  A scalar `Œª` is an *eigenvalue* of `A` if there exists a *non-zero* vector `v` (called an *eigenvector*) such that:
    `Av = Œªv`
    *   **Geometric Interpretation:**  An eigenvector `v` is a vector that, when transformed by `A`, only changes in *magnitude* (scaled by `Œª`), not direction.

*   **Characteristic Equation:** To find the eigenvalues of `A`, solve the *characteristic equation*:
    `det(A - ŒªI) = 0`
    *   This equation is a polynomial equation in `Œª` of degree `n`.  The roots of this polynomial are the eigenvalues of `A`.

*   **Finding Eigenvectors:** For each eigenvalue `Œª`:
    1.  Form the matrix `(A - ŒªI)`.
    2.  Solve the homogeneous system of linear equations `(A - ŒªI)v = 0`.
    3.  The non-zero solutions `v` are the eigenvectors corresponding to `Œª`.  The set of all solutions (including the zero vector) forms the *eigenspace* corresponding to `Œª`, which is a subspace.

*   **Properties of Eigenvalues and Eigenvectors (Extensive List):**
    *   The *sum* of the eigenvalues of `A` is equal to the *trace* of `A` (`tr(A)` = sum of diagonal elements).
    *   The *product* of the eigenvalues of `A` is equal to the *determinant* of `A` (`det(A)`).
    *   If `Œª` is an eigenvalue of `A`, then `Œª·µè` is an eigenvalue of `A·µè` (for any positive integer `k`).
    *   If `Œª` is an eigenvalue of `A`, and `A` is invertible, then `1/Œª` is an eigenvalue of `A‚Åª¬π`.
    *   If `Œª` is an eigenvalue of `A`, then `Œª + c` is an eigenvalue of `A + cI` (where `c` is a scalar and `I` is the identity matrix).
   *   If `Œª` is an eigenvalue of `A`, then  `kŒª` is an eigenvalue of `kA`
    *   **Eigenvalues of Special Matrices:**
        *   **Triangular Matrix (Upper or Lower):** Eigenvalues are the diagonal entries.
        *   **Symmetric Matrix:** Eigenvalues are *real*. Eigenvectors corresponding to *distinct* eigenvalues are *orthogonal*.
        *   **Skew-Symmetric Matrix:** Eigenvalues are either zero or purely imaginary.
        *   **Orthogonal Matrix:** Eigenvalues have absolute value 1 (i.e., they lie on the unit circle in the complex plane). If the matrix is real, the eigenvalues are ¬±1.
        *   **Idempotent Matrix:** Eigenvalues are 0 or 1.
        *   **Nilpotent Matrix:** All eigenvalues are 0.
        * **Singular Matrix:** At least one Eigenvalue is zero.
    *   **Diagonalization:**  An `n x n` matrix `A` is *diagonalizable* if and only if it has `n` linearly independent eigenvectors.  If so, `A = PDP‚Åª¬π`, where:
        *   `D` is a *diagonal matrix* whose diagonal entries are the eigenvalues of `A`.
        *   `P` is an invertible matrix whose *columns* are the corresponding eigenvectors of `A`.  (The order of eigenvectors in `P` must match the order of eigenvalues in `D`).
        *   **Why Diagonalization is Useful:**  It makes computing powers of `A` much easier: `A·µè = PD·µèP‚Åª¬π`.  `D·µè` is trivial to compute (just raise each diagonal entry to the power `k`).
    *   **Cayley-Hamilton Theorem:** *Every* square matrix satisfies its own characteristic equation.
        *   **Example:** If the characteristic equation of `A` is `Œª¬≤ - 5Œª + 6 = 0`, then `A¬≤ - 5A + 6I = 0` (where `0` is the zero matrix).
        *   **Usefulness:** Can be used to find the inverse of a matrix, express higher powers of a matrix in terms of lower powers, and simplify matrix expressions.

* **GATE Problem-Solving (Eigenvalues & Eigenvectors):**
    1.  **Finding Eigenvalues:**
        *   Set up the characteristic equation: `det(A - ŒªI) = 0`.
        *   Solve the characteristic equation for `Œª`. This is usually a polynomial equation.
        *   Use properties (trace, determinant) as checks.  For example, if you find the eigenvalues of a 3x3 matrix to be 1, 2, and 3, check that their sum is equal to the trace of the matrix and their product is equal to the determinant.
    2.  **Finding Eigenvectors:**
        *   For *each* eigenvalue `Œª`, form the matrix `(A - ŒªI)`.
        *   Solve the homogeneous system `(A - ŒªI)v = 0`. You can do this by row reducing `(A - ŒªI)` to echelon form.
        *   Express the solution in terms of free variables (if any). The eigenvectors are the non-zero vectors in this solution space.
    3.  **Diagonalization:**
        *   If the matrix is diagonalizable (has `n` linearly independent eigenvectors), construct `P` (using eigenvectors as columns) and `D` (using eigenvalues as diagonal entries).
        *   Remember that the order of eigenvectors in `P` must correspond to the order of eigenvalues in `D`.
    4.  **Cayley-Hamilton Theorem Problems:** These problems usually involve finding `A‚Åª¬π` or a high power of `A` without direct calculation. Use the characteristic equation to express `A‚Åª¬π` or `A·µè` in terms of lower powers of `A` and the identity matrix.
    5. **Recognize Special Matrices:** Use properties (triangular, symmetric, orthogonal, etc.) to save time.

* **Challenging Concepts:**
    *  **Generalized Eigenvectors:**
    * **Jordan Canonical Form:**

üìå **6. Systems of Linear Equations**

*   **Representation:** `Ax = b`, where:
    *   `A` is the `m x n` *coefficient matrix*.
    *   `x` is the `n x 1` column vector of *unknowns*.
    *   `b` is the `m x 1` column vector of *constants*.

*   **Augmented Matrix:** `[A | b]` (combines the coefficient matrix and the constant vector)

*   **Existence of Solutions:** A solution to `Ax = b` exists if and only if `rank(A) = rank([A | b])`.  This means that `b` must be in the column space of `A`.

*   **Uniqueness of Solutions:**
*   **Uniqueness of Solutions (Continued):**
    *   **Unique Solution:** The system `Ax = b` has a *unique* solution if and only if `rank(A) = rank([A | b]) = n` (where `n` is the number of *unknowns*, i.e., the number of columns of `A`).  This means:
        *   A solution exists.
        *   There are no free variables.
        *   The columns of `A` are linearly independent.
        *   If `A` is square, then `det(A) ‚â† 0` (and `A` is invertible).
    *   **Infinite Solutions:** The system `Ax = b` has *infinitely many* solutions if and only if `rank(A) = rank([A | b]) < n`. This means:
        *   A solution exists.
        *   There are one or more free variables.
        *   The columns of `A` are linearly dependent.
    *   **No Solution:** The system `Ax = b` has *no solution* if and only if `rank(A) < rank([A | b])`. This means:
        *   The system is *inconsistent*.
        *   `b` is *not* in the column space of `A`.
        *   Row reduction of `[A | b]` will lead to a row of the form `[0 0 ... 0 | c]` where `c ‚â† 0`.

*   **Homogeneous Systems (Ax = 0):**
    *   A homogeneous system *always* has at least one solution: the *trivial solution* (`x = 0`).
    *   **Non-trivial Solutions:** A homogeneous system `Ax = 0` has *non-trivial* solutions (solutions other than `x = 0`) if and only if `rank(A) < n` (where `n` is the number of unknowns).  This is equivalent to saying that the columns of `A` are linearly dependent.  If `A` is square, this is also equivalent to `det(A) = 0`.
    * **Fundamental set of Solution:**

*   **Methods for Solving Systems of Linear Equations:**
    *   **Gaussian Elimination (Row Reduction):**
        1.  Form the augmented matrix `[A | b]`.
        2.  Use elementary row operations to transform `[A | b]` into *row echelon form* (REF).
        3.  Check for consistency: If there's a row of the form `[0 0 ... 0 | c]` with `c ‚â† 0`, there's no solution.
        4.  If consistent, use back-substitution to find the solution(s).
    *   **Gauss-Jordan Elimination:**
        1.  Form the augmented matrix `[A | b]`.
        2.  Use elementary row operations to transform `[A | b]` into *reduced row echelon form* (RREF).
        3.  Check for consistency.
        4.  Read off the solution(s) directly from the RREF.  Free variables (if any) will be parameters in the solution.
    *   **Cramer's Rule:**
        *   *Only* applicable when `A` is a *square* matrix and `det(A) ‚â† 0` (i.e., a unique solution exists).
        *   Let `A·µ¢` be the matrix obtained by replacing the `i`-th column of `A` with the column vector `b`.
        *   The solution is given by: `x·µ¢ = det(A·µ¢) / det(A)` for each `i = 1, 2, ..., n`.
        *   **Note:** Cramer's Rule is generally *not* computationally efficient for large systems. It's more useful for theoretical purposes and for small systems (2x2, 3x3).
    *   **LU Decomposition:**
        *   Factor the matrix `A` into a product of a lower triangular matrix `L` and an upper triangular matrix `U`: `A = LU`.
        *   Solve the system `Ax = b` in two steps:
            1.  Solve `Ly = b` for `y` (using forward substitution).
            2.  Solve `Ux = y` for `x` (using backward substitution).
        *   **Advantages:**  LU decomposition is useful when you need to solve `Ax = b` for multiple different `b` vectors. Once you have the LU decomposition, solving for each `b` is relatively fast.
    *  **Inverse Matrix Method:**
     * Applicable for the square matrix.
     *  `X= A‚Åª¬πb`

*   **GATE Problem-Solving (Systems of Linear Equations):**
    1.  **Existence and Uniqueness:**  Use rank conditions: `rank(A) = rank([A | b])` for existence, and `rank(A) = n` for uniqueness.  Row reduction is the primary tool.
    2.  **Solving:**
        *   Gaussian elimination (row reduction to REF) is generally the most versatile and efficient method for GATE problems.
        *   Gauss-Jordan elimination (row reduction to RREF) is also good, as it directly gives you the solution without back-substitution.
        *   Cramer's rule is useful for 2x2 and 3x3 systems, and for theoretical questions where you need to express the solution in terms of determinants.
        *   LU decomposition is less frequently asked about directly in GATE, but understanding the concept is helpful.
    3.  **Homogeneous Systems:**  Remember that they always have the trivial solution. Focus on determining if non-trivial solutions exist (`rank(A) < n`).
    4. **Number of solutions:**

*   **Challenging Concepts & Examples:**
    *   **Overdetermined Systems:** More equations than unknowns (`m > n`).  Usually no exact solution; often solved using least squares.
    *   **Underdetermined Systems:** Fewer equations than unknowns (`m < n`).  Usually infinitely many solutions (if consistent).
    *  **Pseudoinverse (Moore-Penrose Inverse):**
    * **Least Squares Solutions:**  If `Ax = b` has no solution, the least squares solution minimizes the error `||Ax - b||¬≤`.  The least squares solution is found by solving the *normal equations*: `A·µÄAx = A·µÄb`.  If the columns of `A` are linearly independent, the solution is `x = (A·µÄA)‚Åª¬πA·µÄb`.

üìå **7. Challenging Concepts Across Linear Algebra**

* **Direct Sums**
* **Quotient Spaces**
* **Four Fundamental Subspaces (Detailed):**
    *   **Column Space (C(A)):**  The span of the columns of `A`.  Dimension = `rank(A)`.  A basis is found by taking the pivot columns of the *original* matrix `A` after row reduction.  Represents all possible `b` vectors for which `Ax = b` has a solution. Subspace of R·µê.
    *   **Null Space (N(A)):**  The set of all solutions to `Ax = 0`.  Dimension = `nullity(A) = n - rank(A)`.  A basis is found by solving `Ax = 0` and expressing the solution in terms of free variables. Subspace of R‚Åø.
    *   **Row Space (C(A·µÄ)):**  The span of the rows of `A`.  Dimension = `rank(A)`.  A basis is found by taking the non-zero rows of the row echelon form of `A`. Subspace of R‚Åø.
    *   **Left Null Space (N(A·µÄ)):**  The set of all solutions to `A·µÄy = 0`.  Dimension = `m - rank(A)`.  Subspace of R·µê.
    *   **Orthogonality Relationships:**
        *   `N(A)` is orthogonal to `C(A·µÄ)` (Nullspace is orthogonal to Row space)
        *   `N(A·µÄ)` is orthogonal to `C(A)` (Left nullspace is orthogonal to Column space)

*   **Change of Basis:**
    *   If you have a vector's coordinates with respect to one basis, you can find its coordinates with respect to another basis using a *change-of-basis matrix*.
    *   Let `B = {b‚ÇÅ, b‚ÇÇ, ..., b‚Çô}` and `C = {c‚ÇÅ, c‚ÇÇ, ..., c‚Çô}` be two bases for a vector space `V`.  The change-of-basis matrix from `B` to `C`, denoted `P_(C<-B)`, transforms coordinate vectors with respect to `B` into coordinate vectors with respect to `C`:
        `[v]_C = P_(C<-B) [v]_B`
    *   The columns of `P_(C<-B)` are the coordinate vectors of the vectors in `B` with respect to the basis `C`.  That is, the `j`-th column of `P_(C<-B)` is `[b‚±º]_C`.
    *  `P_(B<-C) = (P_(C<-B))‚Åª¬π`

* **LU Decomposition (Detailed):**
     * **L:** Unit Lower Triangular Matrix (1s on the diagonal)
    *   **U:** Upper Triangular Matrix (the echelon form of A)
    *   **Existence:**  Not all matrices have an LU decomposition.  A sufficient condition is that all leading principal minors of `A` are non-zero.
    *   **Finding L and U:**  Gaussian elimination.  `L` keeps track of the row operations used to reduce `A` to `U`.

*  **QR Decomposition:**
    *  A = QR where Q is orthogonal matrix.
    * R is upper triangular matrix.

*   **Singular Value Decomposition (SVD):**  Any matrix `A` can be decomposed as `A = UŒ£V·µÄ`, where:
    *   `U` is an orthogonal matrix whose columns are the *left singular vectors* of `A`.
    *   `Œ£` is a diagonal matrix with the *singular values* of `A` (which are the square roots of the eigenvalues of `A·µÄA`) on the diagonal.
    *   `V` is an orthogonal matrix whose columns are the *right singular vectors* of `A`.
    *  **Applications:**  Dimensionality reduction (PCA), image compression, solving least squares problems, finding the pseudoinverse.

*   **Pseudoinverse (Moore-Penrose Inverse):**  A generalization of the inverse for non-square and singular matrices.  Denoted `A‚Å∫`.
    *  If `A` has full column rank then `A‚Å∫ = (A·µÄA)‚Åª¬πA·µÄ`
    * If A has full row rank then `A‚Å∫ = A·µÄ(AA·µÄ)‚Åª¬π`
    * If A is square and invertible then `A‚Å∫=A‚Åª¬π`
    *  Can be used to find least square solution.

*  **Least Squares:**

üöÄ **Ultimate GATE Linear Algebra Master Strategy**

1.  **Deep Understanding:**  Don't just memorize formulas; understand the *why* behind them.  Know the definitions and theorems.
2.  **Row Reduction is Your Best Friend:**  Master row reduction (Gaussian and Gauss-Jordan elimination).  It's the key to solving a huge variety of problems.
3.  **Special Matrices:**  Learn to recognize special matrix types (symmetric, orthogonal, triangular, etc.) and immediately apply their properties.
4.  **Eigenvalues and Eigenvectors:**  Understand their geometric meaning and their connection to matrix properties. Practice finding them efficiently.
5.  **Rank, Nullity, and the Four Fundamental Subspaces:**  These concepts are deeply interconnected.  Understand the relationships between them.
6.  **Systems of Equations:**  Master the conditions for existence and uniqueness of solutions.  Know how to solve different types of systems.
7.  **Practice, Practice, Practice:**  Solve a wide variety of problems, including previous GATE questions.  Focus on understanding the *concepts* and *problem-solving strategies*, not just memorizing formulas.  Work through problems step-by-step, and try to anticipate the solution before you start calculating.
8. **Visualization:** Try to visualize the concepts as much as possible. Imagine vectors, subspaces, and transformations geometrically.
